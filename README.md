# Document Similarity using Hadoop Streaming

---

## ðŸ“Œ Project Title

**Document Similarity â€“ Compute Cosine Similarity between Documents using Word Vectors**

---

## ðŸ“‚ Dataset

- **Dataset Source:** [Kaggle - 20 Newsgroups Dataset](https://www.kaggle.com/datasets/crawford/20-newsgroups)
- The dataset contains text data from 20 different newsgroups which is widely used for text classification and NLP tasks.

---

## ðŸŽ¯ Objective

The objective of this project is to:

- Build a text similarity system using cosine similarity.
- Process large-scale text data using **Hadoop Streaming** with **Python**.
- Use **TF-IDF (Term Frequency - Inverse Document Frequency)** for vector representation of documents.
- Calculate pairwise cosine similarity across documents.
- Run the entire solution in distributed fashion using **HDFS** and **YARN**.

---

## ðŸ›  Technologies Used

- **Hadoop 3.3.6 (Standalone Mode on WSL2 - Windows Subsystem for Linux 2)**
- **HDFS - Hadoop Distributed File System**
- **YARN - Yet Another Resource Negotiator**
- **Hadoop Streaming (for running Python scripts)**
- **Python 3.x**
- **Virtual Environment (venv) for package isolation**
- **scikit-learn** - for TF-IDF vectorization
- **nltk** - for text preprocessing (stopwords removal)
- **WSL2 (Ubuntu 24.04 on Windows 11)**

---

## ðŸ“‘ Project Folder Structure

```bash
DocumentSimilarity-Hadoop/
â”‚
â”œâ”€â”€ 20_newsgroups/        # Raw dataset files (optional, downloaded from Kaggle)
â”œâ”€â”€ dataset/              # Cleaned dataset after preprocessing
â”‚   â””â”€â”€ cleaned_data.txt
â”œâ”€â”€ scripts/              # All Python source codes
â”‚   â”œâ”€â”€ preprocess.py     # Dataset cleaning & preprocessing
â”‚   â””â”€â”€ mapper.py         # Hadoop Streaming mapper (cosine similarity calculation)
â”‚   â””â”€â”€ reducer.py        # (optional, not used in final version)
â”œâ”€â”€ venv.tar.gz           # Packaged virtual environment for Hadoop Streaming
â”œâ”€â”€ results/              # Sample output files (optional)
â”œâ”€â”€ logs/                 # Hadoop job logs/screenshots (optional)
â”œâ”€â”€ README.md             # Project documentation
â””â”€â”€ .git/                 # Git repository

